{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90hkQbFgk3G3",
        "outputId": "dfa0d50b-2be4-4a2a-ca44-215aab515716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "file_path='/content/drive/MyDrive/Kidney_segmentation_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlu6Sc__lfcq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import albumentations as A\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense\n",
        "#from keras.layers import Flatten\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Lambda\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFaLTSHAmB84"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from keras.optimizers import Adam\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQbc-rKCmIam"
      },
      "outputs": [],
      "source": [
        "# Enable mixed precision and tensor float 32 execution\n",
        "tf.config.experimental.enable_mixed_precision = True\n",
        "tf.config.experimental.enable_tensor_float_32_execution = True\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VcxHY0pmlub",
        "outputId": "e31e61ed-3eb0-464f-c6cc-c9a9f77dc6e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(physical_devices)\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCZNDMlCofz-"
      },
      "outputs": [],
      "source": [
        "def Augmentation(images,mask):\n",
        "    def _transform(image,mask):\n",
        "        transform=A.Compose([\n",
        "         A.HorizontalFlip(p=0.25),\n",
        "         A.VerticalFlip(p=0.25),\n",
        "         A.RandomRotate90(p=.5),\n",
        "         A.RandomContrast(limit=0.2, p=0.1),\n",
        "         A.Transpose(p=0.5),\n",
        "         A.ElasticTransform(p=.4, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "    ])\n",
        "        aug=transform(image=image,mask=mask)\n",
        "        img=aug['image']\n",
        "        img=tf.cast(img,tf.float64)\n",
        "        msk=aug['mask']\n",
        "        msk=tf.cast(msk,tf.float64)\n",
        "#         print(\"after augmentation\",img.shape,msk.shape)\n",
        "        return img,msk\n",
        "    image,mask=tf.numpy_function(_transform,[images,mask],[tf.float64,tf.float64])\n",
        "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    mask.set_shape([IMAGE_SIZE, IMAGE_SIZE, 2])\n",
        "    return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAy-Es8Foh4I",
        "outputId": "b8067cfb-8e48-4b78-f7be-fdbb746439ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE=256\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "def load_data(split=0.3):\n",
        "    images = sorted(glob(file_path + \"batch_256/*jpg\"), key=lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
        "    masks = sorted(glob(file_path+\"masks_256/*\"))\n",
        "    total_size = len(images)\n",
        "    valid_size = int(split * total_size)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split, random_state=42)\n",
        "\n",
        "    valid_x, test_x = train_test_split(valid_x, test_size=split, random_state=42)\n",
        "    valid_y, test_y = train_test_split(valid_y, test_size=split, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y),(test_x,test_y)\n",
        "(train_x,train_y),(valid_x,valid_y),(test_x,test_y)=load_data()\n",
        "train_size=len(train_x)\n",
        "valid_size=len(valid_x)\n",
        "\n",
        "\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x,(256,256),interpolation=cv2.INTER_AREA)\n",
        "    x = x/255.0\n",
        "    x=tf.cast(x,dtype=tf.float64)\n",
        "    return x\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x,(256,256),interpolation=cv2.INTER_NEAREST)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = np.concatenate([x,x],axis=-1)\n",
        "    x=tf.cast(x,dtype=tf.float64)\n",
        "    return x\n",
        "\n",
        "def parser(x,y):\n",
        "    def _parse(x,y):\n",
        "        x=read_image(x)\n",
        "        y=read_mask(y)\n",
        "        return x,y\n",
        "    x,y = tf.numpy_function(_parse, [x,y], [tf.float64,tf.float64])\n",
        "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 2])\n",
        "    return x,y\n",
        "    \n",
        "def tf_dataset(x, y, batch,repeat_original=1):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.map(parser)\n",
        "    original_dataset = dataset.repeat(repeat_original)\n",
        "\n",
        "    augmented_dataset = dataset.map(Augmentation)\n",
        "    dataset = tf.data.Dataset.concatenate(original_dataset, augmented_dataset)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(128,seed=0)\n",
        "    dataset = dataset.batch(batch)\n",
        "    return dataset\n",
        "train_dataset=tf_dataset(train_x,train_y,batch=8)\n",
        "valid_dataset=tf_dataset(valid_x,valid_y,batch=8)\n",
        "\n",
        "IMAGE_WIDTH=256\n",
        "IMAGE_HEIGHT=256\n",
        "IMAGE_CHANNELS=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7q7axOTove0"
      },
      "outputs": [],
      "source": [
        "#LOSS FUNCTIONS \n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "smooth = 1\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def binary_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "def binary_dice_loss(y_true, y_pred):\n",
        "    d_loss= dice_loss(y_true, y_pred)\n",
        "    b_loss= binary_loss(y_true, y_pred)\n",
        "    loss=(d_loss+b_loss)\n",
        "    return loss\n",
        "\n",
        "def Tversky_Index(y_true,y_pred,smooth):\n",
        "    y_true= tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred= tf.keras.layers.Flatten()(y_pred)\n",
        "    true_positive= tf.reduce_sum(y_true*y_pred)\n",
        "    false_negative=tf.reduce_sum(y_true*(1-y_pred))\n",
        "    false_positive=tf.reduce_sum((1-y_true)*y_pred)\n",
        "    alpha=.7\n",
        "    TI=(true_positive+smooth)/(true_positive+alpha*false_negative+(1-alpha)*false_positive+smooth)\n",
        "    return TI\n",
        "\n",
        "def Tversky_loss(y_true,y_pred):\n",
        "    return 1-Tversky_Index(y_true,y_pred,smooth=1)\n",
        "def focal_Tversky_loss(y_true,y_pred):\n",
        "    pt_1 = Tversky_Index(y_true, y_pred,smooth=1)\n",
        "    gamma = 0.75\n",
        "    return tf.math.pow((1-pt_1), gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjzJOTmboj9y"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.layers import GlobalAveragePooling2D \n",
        "from keras.layers import multiply, Reshape\n",
        "from tensorflow.keras.optimizers import Nadam ,Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRWejrSbozIE"
      },
      "outputs": [],
      "source": [
        "#models\n",
        "def squeeze_excite_block(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, filters):\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder1(inputs):\n",
        "    skip_connections = [inputs]\n",
        "\n",
        "    model = tf.keras.applications.DenseNet169(include_top=False, weights='imagenet',input_tensor=inputs)\n",
        "    for layers in model.layers:\n",
        "        layers.trainable=False\n",
        "        \n",
        "    #model.summary()\n",
        "    names = [\"conv1/relu\", \"pool2_conv\", \"pool3_conv\", \"pool4_conv\"]\n",
        "    for name in names:\n",
        "        skip_connections.append(model.get_layer(name).output)\n",
        "\n",
        "    output = model.get_layer(\"relu\").output\n",
        "    return output, skip_connections\n",
        "\n",
        "def decoder1(inputs, skip_connections):\n",
        "    num_filters = [ 256, 128, 64,64,32]\n",
        "    skip_connections.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = concatenate([x, skip_connections[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "def encoder2(inputs):\n",
        "    num_filters = [32,64,64, 128, 256]\n",
        "    skip_connections = []\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = conv_block(x, f)\n",
        "        skip_connections.append(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    return x, skip_connections\n",
        "\n",
        "def decoder2(inputs, skip_1, skip_2):\n",
        "    num_filters = [256, 128, 64, 64,32]\n",
        "    skip_2.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = concatenate([x, skip_1[i], skip_2[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def output_block(inputs):\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Upsample(tensor, size):\n",
        "    \"\"\"Bilinear upsampling\"\"\"\n",
        "    def _upsample(x, size):\n",
        "        return tf.image.resize(images=x, size=size)\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
        "def ASPP(x, filter):\n",
        "    shape = x.shape\n",
        "\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
        "\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = concatenate([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def build_model(shape):\n",
        "    inputs = Input(shape)\n",
        "    x, skip_1 = encoder1(inputs)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder1(x, skip_1)\n",
        "    outputs1 = output_block(x)\n",
        "\n",
        "    x = inputs * outputs1\n",
        "\n",
        "    x, skip_2 = encoder2(x)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder2(x, skip_1, skip_2)\n",
        "    outputs2 = output_block(x)\n",
        "    outputs = concatenate([outputs1, outputs2])\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRdWnZx5f8F2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model=tf.keras.models.load_model(file_path+\"batch/\"+\"tenth_model.hd5\",custom_objects={\n",
        "        'dice_loss': dice_loss ,'dice_coef': dice_coef})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT1v2sPwo1N3",
        "outputId": "d66c35a6-da6e-43c0-ecb9-f8f9f76069ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "822/838 [============================>.] - ETA: 3:04 - loss: 0.1626 - dice_coef: 0.8374 - recall: 0.8492 - precision: 0.9178"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.1622 - dice_coef: 0.8378 - recall: 0.8486 - precision: 0.9181 \n",
            "Epoch 1: val_dice_coef improved from -inf to 0.79796, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch/eleventh_model.hd5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 12822s 15s/step - loss: 0.1622 - dice_coef: 0.8378 - recall: 0.8486 - precision: 0.9181 - val_loss: 0.2020 - val_dice_coef: 0.7980 - val_recall: 0.8329 - val_precision: 0.9001\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2292 - dice_coef: 0.7708 - recall: 0.7760 - precision: 0.8735\n",
            "Epoch 2: val_dice_coef improved from 0.79796 to 0.81632, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch/eleventh_model.hd5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - 1639s 2s/step - loss: 0.2292 - dice_coef: 0.7708 - recall: 0.7760 - precision: 0.8735 - val_loss: 0.1837 - val_dice_coef: 0.8163 - val_recall: 0.8369 - val_precision: 0.8921\n",
            "Epoch 3/20\n",
            "820/838 [============================>.] - ETA: 28s - loss: 0.1622 - dice_coef: 0.8378 - recall: 0.8532 - precision: 0.9125"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.1616 - dice_coef: 0.8384 - recall: 0.8534 - precision: 0.9124\n",
            "Epoch 3: val_dice_coef improved from 0.81632 to 0.82457, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch/eleventh_model.hd5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1571s 2s/step - loss: 0.1616 - dice_coef: 0.8384 - recall: 0.8534 - precision: 0.9124 - val_loss: 0.1754 - val_dice_coef: 0.8246 - val_recall: 0.8417 - val_precision: 0.8948\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2227 - dice_coef: 0.7773 - recall: 0.7915 - precision: 0.8731\n",
            "Epoch 4: val_dice_coef did not improve from 0.82457\n",
            "838/838 [==============================] - 1579s 2s/step - loss: 0.2227 - dice_coef: 0.7773 - recall: 0.7915 - precision: 0.8731 - val_loss: 0.1934 - val_dice_coef: 0.8066 - val_recall: 0.8142 - val_precision: 0.9201\n",
            "Epoch 5/20\n",
            "255/838 [========>.....................] - ETA: 14:47 - loss: 0.1627 - dice_coef: 0.8373 - recall: 0.8520 - precision: 0.9181"
          ]
        }
      ],
      "source": [
        "# model=build_model((256,256,3))\n",
        "\n",
        "device = '/GPU:0'\n",
        "save_filepath= file_path+\"batch/\"+\"eleventh_model.hd5\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10),\n",
        "    ModelCheckpoint(save_filepath,verbose=1,monitor='val_dice_coef',mode='max',save_best_only=True),\n",
        "    CSVLogger(file_path+\"batch/\"+\"dataDensenet161freeze.csv\")\n",
        "    ]\n",
        "with tf.device(device):\n",
        "    model.compile(loss=dice_loss,optimizer=Adam(learning_rate=0.00025),metrics=[dice_coef,Recall(),Precision()])\n",
        "    t_steps=train_size//8\n",
        "    v_steps=valid_size//8\n",
        "with tf.device(device):\n",
        "    history=model.fit(train_dataset, \n",
        "                            epochs =20,\n",
        "                            steps_per_epoch = t_steps,\n",
        "                            validation_data = valid_dataset,\n",
        "                            validation_steps = v_steps,\n",
        "                            callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChFwwGJ-EZX2"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}