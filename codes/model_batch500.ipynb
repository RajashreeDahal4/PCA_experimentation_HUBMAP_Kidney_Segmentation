{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90hkQbFgk3G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217547b0-a8dc-460b-b139-f2e7b51fa6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)\n",
        "file_path='/content/drive/MyDrive/Kidney_segmentation_data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlu6Sc__lfcq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import albumentations as A\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense\n",
        "#from keras.layers import Flatten\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, AveragePooling2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Lambda\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFaLTSHAmB84"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import CSVLogger\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from keras.optimizers import Adam\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQbc-rKCmIam"
      },
      "outputs": [],
      "source": [
        "# Enable mixed precision and tensor float 32 execution\n",
        "tf.config.experimental.enable_mixed_precision = True\n",
        "tf.config.experimental.enable_tensor_float_32_execution = True\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VcxHY0pmlub",
        "outputId": "36e244f7-9ce9-41e4-f1ce-c90ef8c1536e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "print(physical_devices)\n",
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCZNDMlCofz-"
      },
      "outputs": [],
      "source": [
        "def Augmentation(images,mask):\n",
        "    def _transform(image,mask):\n",
        "        transform=A.Compose([\n",
        "         A.HorizontalFlip(p=0.25),\n",
        "         A.VerticalFlip(p=0.25),\n",
        "         A.RandomRotate90(p=.5),\n",
        "         A.RandomContrast(limit=0.2, p=0.1),\n",
        "         A.Transpose(p=0.5),\n",
        "         A.ElasticTransform(p=.4, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
        "    ])\n",
        "        aug=transform(image=image,mask=mask)\n",
        "        img=aug['image']\n",
        "        img=tf.cast(img,tf.float64)\n",
        "        msk=aug['mask']\n",
        "        msk=tf.cast(msk,tf.float64)\n",
        "#         print(\"after augmentation\",img.shape,msk.shape)\n",
        "        return img,msk\n",
        "    image,mask=tf.numpy_function(_transform,[images,mask],[tf.float64,tf.float64])\n",
        "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    mask.set_shape([IMAGE_SIZE, IMAGE_SIZE, 2])\n",
        "    return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAy-Es8Foh4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7967e0d8-4199-4f58-f1ab-487178ed0a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE=256\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "def load_data(split=0.3):\n",
        "    images = sorted(glob(file_path + \"batch_500_256/*jpg\"), key=lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
        "    masks = sorted(glob(file_path+\"masks_256/*\"))\n",
        "    total_size = len(images)\n",
        "    valid_size = int(split * total_size)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split, random_state=42)\n",
        "    train_y, valid_y = train_test_split(masks, test_size=split, random_state=42)\n",
        "\n",
        "    valid_x, test_x = train_test_split(valid_x, test_size=split, random_state=42)\n",
        "    valid_y, test_y = train_test_split(valid_y, test_size=split, random_state=42)\n",
        "\n",
        "    return (train_x, train_y), (valid_x, valid_y),(test_x,test_y)\n",
        "(train_x,train_y),(valid_x,valid_y),(test_x,test_y)=load_data()\n",
        "train_size=len(train_x)\n",
        "valid_size=len(valid_x)\n",
        "\n",
        "\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x,(256,256),interpolation=cv2.INTER_AREA)\n",
        "    x = x/255.0\n",
        "    x=tf.cast(x,dtype=tf.float64)\n",
        "    return x\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = cv2.resize(x,(256,256),interpolation=cv2.INTER_NEAREST)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    x = np.concatenate([x,x],axis=-1)\n",
        "    x=tf.cast(x,dtype=tf.float64)\n",
        "    return x\n",
        "\n",
        "def parser(x,y):\n",
        "    def _parse(x,y):\n",
        "        x=read_image(x)\n",
        "        y=read_mask(y)\n",
        "        return x,y\n",
        "    x,y = tf.numpy_function(_parse, [x,y], [tf.float64,tf.float64])\n",
        "    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
        "    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 2])\n",
        "    return x,y\n",
        "    \n",
        "def tf_dataset(x, y, batch,repeat_original=1):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.map(parser)\n",
        "    original_dataset = dataset.repeat(repeat_original)\n",
        "\n",
        "    augmented_dataset = dataset.map(Augmentation)\n",
        "    dataset = tf.data.Dataset.concatenate(original_dataset, augmented_dataset)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(128,seed=0)\n",
        "    dataset = dataset.batch(batch)\n",
        "    return dataset\n",
        "train_dataset=tf_dataset(train_x,train_y,batch=8)\n",
        "valid_dataset=tf_dataset(valid_x,valid_y,batch=8)\n",
        "\n",
        "IMAGE_WIDTH=256\n",
        "IMAGE_HEIGHT=256\n",
        "IMAGE_CHANNELS=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7q7axOTove0"
      },
      "outputs": [],
      "source": [
        "#LOSS FUNCTIONS \n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "smooth = 1\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "def binary_loss(y_true, y_pred):\n",
        "    return binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "def binary_dice_loss(y_true, y_pred):\n",
        "    d_loss= dice_loss(y_true, y_pred)\n",
        "    b_loss= binary_loss(y_true, y_pred)\n",
        "    loss=(d_loss+b_loss)\n",
        "    return loss\n",
        "\n",
        "def Tversky_Index(y_true,y_pred,smooth):\n",
        "    y_true= tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred= tf.keras.layers.Flatten()(y_pred)\n",
        "    true_positive= tf.reduce_sum(y_true*y_pred)\n",
        "    false_negative=tf.reduce_sum(y_true*(1-y_pred))\n",
        "    false_positive=tf.reduce_sum((1-y_true)*y_pred)\n",
        "    alpha=.7\n",
        "    TI=(true_positive+smooth)/(true_positive+alpha*false_negative+(1-alpha)*false_positive+smooth)\n",
        "    return TI\n",
        "\n",
        "def Tversky_loss(y_true,y_pred):\n",
        "    return 1-Tversky_Index(y_true,y_pred,smooth=1)\n",
        "def focal_Tversky_loss(y_true,y_pred):\n",
        "    pt_1 = Tversky_Index(y_true, y_pred,smooth=1)\n",
        "    gamma = 0.75\n",
        "    return tf.math.pow((1-pt_1), gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjzJOTmboj9y"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG19\n",
        "from keras.layers import GlobalAveragePooling2D \n",
        "from keras.layers import multiply, Reshape\n",
        "from tensorflow.keras.optimizers import Nadam ,Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRWejrSbozIE"
      },
      "outputs": [],
      "source": [
        "#models\n",
        "def squeeze_excite_block(inputs, ratio=8):\n",
        "    init = inputs\n",
        "    channel_axis = -1\n",
        "    filters = init.shape[channel_axis]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
        "\n",
        "    x = multiply([init, se])\n",
        "    return x\n",
        "\n",
        "def conv_block(inputs, filters):\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters, (3, 3), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = squeeze_excite_block(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder1(inputs):\n",
        "    skip_connections = [inputs]\n",
        "\n",
        "    model = tf.keras.applications.DenseNet169(include_top=False, weights='imagenet',input_tensor=inputs)\n",
        "    for layers in model.layers:\n",
        "        layers.trainable=False\n",
        "        \n",
        "    #model.summary()\n",
        "    names = [\"conv1/relu\", \"pool2_conv\", \"pool3_conv\", \"pool4_conv\"]\n",
        "    for name in names:\n",
        "        skip_connections.append(model.get_layer(name).output)\n",
        "\n",
        "    output = model.get_layer(\"relu\").output\n",
        "    return output, skip_connections\n",
        "\n",
        "def decoder1(inputs, skip_connections):\n",
        "    num_filters = [ 256, 128, 64,64,32]\n",
        "    skip_connections.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = concatenate([x, skip_connections[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "def encoder2(inputs):\n",
        "    num_filters = [32,64,64, 128, 256]\n",
        "    skip_connections = []\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = conv_block(x, f)\n",
        "        skip_connections.append(x)\n",
        "        x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    return x, skip_connections\n",
        "\n",
        "def decoder2(inputs, skip_1, skip_2):\n",
        "    num_filters = [256, 128, 64, 64,32]\n",
        "    skip_2.reverse()\n",
        "    x = inputs\n",
        "\n",
        "    for i, f in enumerate(num_filters):\n",
        "        x = UpSampling2D((2, 2), interpolation='bilinear')(x)\n",
        "        x = concatenate([x, skip_1[i], skip_2[i]])\n",
        "        x = conv_block(x, f)\n",
        "\n",
        "    return x\n",
        "\n",
        "def output_block(inputs):\n",
        "    x = Conv2D(1, (1, 1), padding=\"same\")(inputs)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def Upsample(tensor, size):\n",
        "    \"\"\"Bilinear upsampling\"\"\"\n",
        "    def _upsample(x, size):\n",
        "        return tf.image.resize(images=x, size=size)\n",
        "    return Lambda(lambda x: _upsample(x, size), output_shape=size)(tensor)\n",
        "def ASPP(x, filter):\n",
        "    shape = x.shape\n",
        "\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n",
        "    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation='bilinear')(y1)\n",
        "\n",
        "    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    y3 = Conv2D(filter, 3, dilation_rate=6, padding=\"same\", use_bias=False)(x)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    y4 = Conv2D(filter, 3, dilation_rate=12, padding=\"same\", use_bias=False)(x)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    y5 = Conv2D(filter, 3, dilation_rate=18, padding=\"same\", use_bias=False)(x)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = concatenate([y1, y2, y3, y4, y5])\n",
        "\n",
        "    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def build_model(shape):\n",
        "    inputs = Input(shape)\n",
        "    x, skip_1 = encoder1(inputs)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder1(x, skip_1)\n",
        "    outputs1 = output_block(x)\n",
        "\n",
        "    x = inputs * outputs1\n",
        "\n",
        "    x, skip_2 = encoder2(x)\n",
        "    x = ASPP(x, 64)\n",
        "    x = decoder2(x, skip_1, skip_2)\n",
        "    outputs2 = output_block(x)\n",
        "    outputs = concatenate([outputs1, outputs2])\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRdWnZx5f8F2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model=tf.keras.models.load_model(file_path+\"batch_500/\"+\"sixth_model.hd5\",custom_objects={\n",
        "        'dice_loss': dice_loss ,'dice_coef': dice_coef})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT1v2sPwo1N3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e290a825-67e7-4d98-dfd4-4c280bc754f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51877672/51877672 [==============================] - 0s 0us/step\n",
            "Epoch 1/20\n",
            "821/838 [============================>.] - ETA: 27s - loss: 0.7309 - dice_coef: 0.2691 - recall: 0.7844 - precision: 0.2513"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.7255 - dice_coef: 0.2745 - recall: 0.7819 - precision: 0.2544\n",
            "Epoch 1: val_dice_coef improved from -inf to 0.51892, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 2035s 2s/step - loss: 0.7255 - dice_coef: 0.2745 - recall: 0.7819 - precision: 0.2544 - val_loss: 0.4811 - val_dice_coef: 0.5189 - val_recall: 0.6865 - val_precision: 0.7717\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.4288 - dice_coef: 0.5712 - recall: 0.6442 - precision: 0.6398\n",
            "Epoch 2: val_dice_coef improved from 0.51892 to 0.70270, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1575s 2s/step - loss: 0.4288 - dice_coef: 0.5712 - recall: 0.6442 - precision: 0.6398 - val_loss: 0.2973 - val_dice_coef: 0.7027 - val_recall: 0.7773 - val_precision: 0.7579\n",
            "Epoch 3/20\n",
            "818/838 [============================>.] - ETA: 29s - loss: 0.2853 - dice_coef: 0.7147 - recall: 0.7330 - precision: 0.8095"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2867 - dice_coef: 0.7133 - recall: 0.7304 - precision: 0.8103\n",
            "Epoch 3: val_dice_coef did not improve from 0.70270\n",
            "838/838 [==============================] - 1396s 2s/step - loss: 0.2867 - dice_coef: 0.7133 - recall: 0.7304 - precision: 0.8103 - val_loss: 0.4596 - val_dice_coef: 0.5404 - val_recall: 0.4398 - val_precision: 0.9190\n",
            "Epoch 4/20\n",
            "838/838 [==============================] - ETA: 0s - loss: 0.3268 - dice_coef: 0.6732 - recall: 0.6801 - precision: 0.7770\n",
            "Epoch 4: val_dice_coef improved from 0.70270 to 0.72821, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1569s 2s/step - loss: 0.3268 - dice_coef: 0.6732 - recall: 0.6801 - precision: 0.7770 - val_loss: 0.2718 - val_dice_coef: 0.7282 - val_recall: 0.7199 - val_precision: 0.8888\n",
            "Epoch 5/20\n",
            "819/838 [============================>.] - ETA: 28s - loss: 0.2349 - dice_coef: 0.7651 - recall: 0.7740 - precision: 0.8636"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2352 - dice_coef: 0.7648 - recall: 0.7733 - precision: 0.8641\n",
            "Epoch 5: val_dice_coef did not improve from 0.72821\n",
            "838/838 [==============================] - 1409s 2s/step - loss: 0.2352 - dice_coef: 0.7648 - recall: 0.7733 - precision: 0.8641 - val_loss: 0.3106 - val_dice_coef: 0.6894 - val_recall: 0.6197 - val_precision: 0.9268\n",
            "Epoch 6/20\n",
            "838/838 [==============================] - ETA: 0s - loss: 0.3040 - dice_coef: 0.6960 - recall: 0.6873 - precision: 0.8213\n",
            "Epoch 6: val_dice_coef improved from 0.72821 to 0.76958, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1606s 2s/step - loss: 0.3040 - dice_coef: 0.6960 - recall: 0.6873 - precision: 0.8213 - val_loss: 0.2304 - val_dice_coef: 0.7696 - val_recall: 0.7549 - val_precision: 0.9003\n",
            "Epoch 7/20\n",
            "819/838 [============================>.] - ETA: 28s - loss: 0.2268 - dice_coef: 0.7732 - recall: 0.7823 - precision: 0.8656"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2270 - dice_coef: 0.7730 - recall: 0.7817 - precision: 0.8664\n",
            "Epoch 7: val_dice_coef improved from 0.76958 to 0.77767, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1510s 2s/step - loss: 0.2270 - dice_coef: 0.7730 - recall: 0.7817 - precision: 0.8664 - val_loss: 0.2223 - val_dice_coef: 0.7777 - val_recall: 0.7831 - val_precision: 0.8951\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2801 - dice_coef: 0.7199 - recall: 0.7275 - precision: 0.8134\n",
            "Epoch 8: val_dice_coef improved from 0.77767 to 0.78581, saving model to /content/drive/MyDrive/Kidney_segmentation_data/batch_500/seventh_model.hd5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 213). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r838/838 [==============================] - 1573s 2s/step - loss: 0.2801 - dice_coef: 0.7199 - recall: 0.7275 - precision: 0.8134 - val_loss: 0.2142 - val_dice_coef: 0.7858 - val_recall: 0.7854 - val_precision: 0.8954\n",
            "Epoch 9/20\n",
            "820/838 [============================>.] - ETA: 27s - loss: 0.2177 - dice_coef: 0.7823 - recall: 0.8039 - precision: 0.8838"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "838/838 [==============================] - ETA: 0s - loss: 0.2162 - dice_coef: 0.7838 - recall: 0.8040 - precision: 0.8849\n",
            "Epoch 9: val_dice_coef did not improve from 0.78581\n",
            "838/838 [==============================] - 1396s 2s/step - loss: 0.2162 - dice_coef: 0.7838 - recall: 0.8040 - precision: 0.8849 - val_loss: 0.2409 - val_dice_coef: 0.7591 - val_recall: 0.7570 - val_precision: 0.9223\n",
            "Epoch 10/20\n",
            "316/838 [==========>...................] - ETA: 13:47 - loss: 0.2824 - dice_coef: 0.7176 - recall: 0.7262 - precision: 0.8210"
          ]
        }
      ],
      "source": [
        "model=build_model((256,256,3))\n",
        "\n",
        "device = '/GPU:0'\n",
        "save_filepath= file_path+\"batch_500/\"+\"seventh_model.hd5\"\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10),\n",
        "    ModelCheckpoint(save_filepath,verbose=1,monitor='val_dice_coef',mode='max',save_best_only=True),\n",
        "    CSVLogger(file_path+\"batch_500/\"+\"dataDensenet161freeze.csv\")\n",
        "    ]\n",
        "with tf.device(device):\n",
        "    model.compile(loss=dice_loss,optimizer=Adam(learning_rate=0.00025),metrics=[dice_coef,Recall(),Precision()])\n",
        "    t_steps=train_size//8\n",
        "    v_steps=valid_size//8\n",
        "with tf.device(device):\n",
        "    history=model.fit(train_dataset, \n",
        "                            epochs =20,\n",
        "                            steps_per_epoch = t_steps,\n",
        "                            validation_data = valid_dataset,\n",
        "                            validation_steps = v_steps,\n",
        "                            callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ChFwwGJ-EZX2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}